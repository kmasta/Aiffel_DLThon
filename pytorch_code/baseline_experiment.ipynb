{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kmasta/Aiffel_DLThon.git"
      ],
      "metadata": {
        "id": "NKgmWhR0fg9a"
      },
      "id": "NKgmWhR0fg9a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "eb181ba8",
      "metadata": {
        "id": "eb181ba8"
      },
      "source": [
        "# Baseline Code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df08a157",
      "metadata": {
        "id": "df08a157"
      },
      "source": [
        "## í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "145d5e65",
      "metadata": {
        "id": "145d5e65"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback, ElectraForSequenceClassification\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import os\n",
        "\n",
        "from Aiffel_DLThon.pytorch_code.callback.logger import log_message, log_target_distribution, LoggingCallback\n",
        "from Aiffel_DLThon.pytorch_code.callback.early_stopping import EarlyStopping\n",
        "from Aiffel_DLThon.pytorch_code.callback.checkpoint import save_model\n",
        "from Aiffel_DLThon.pytorch_code.callback.save_results import save_submission"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4fe5cd0",
      "metadata": {
        "id": "f4fe5cd0"
      },
      "source": [
        "## ëª¨ë¸ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76cf4acc",
      "metadata": {
        "id": "76cf4acc"
      },
      "outputs": [],
      "source": [
        "# Load model name\n",
        "with open(\"./Aiffel_DLThon/pytorch_code/config/model_name.yaml\", \"r\") as f:\n",
        "    model_name_config = yaml.safe_load(f)\n",
        "model_key = model_name_config[\"model_name\"]\n",
        "\n",
        "# Load full config for that model\n",
        "with open(f\"./Aiffel_DLThon/pytorch_code/config/{model_key}.yaml\", \"r\") as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "log_message(f\"Running experiment: {config['experiment_name']}\", config[\"log_dir\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96c16e5e",
      "metadata": {
        "id": "96c16e5e"
      },
      "source": [
        "## í† í¬ë‚˜ì´ì €, ëª¨ë¸ êµ¬í˜„"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ElectraPreTrainedModel, ElectraModel\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "class ElectraTwoStageModel(ElectraPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.electra = ElectraModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.stage1_classifier = nn.Linear(config.hidden_size, 2)  # Binary\n",
        "        self.stage2_classifier = nn.Linear(config.hidden_size, 4)  # Multiclass\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None,\n",
        "            binary_labels=None, multiclass_labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "        cls_output = self.dropout(outputs.last_hidden_state[:, 0, :])\n",
        "\n",
        "        # Stage 1: ì¼ë°˜ vs íŠ¹ìˆ˜\n",
        "        stage1_logits = self.stage1_classifier(cls_output)\n",
        "\n",
        "        # Stage 2: ì¼ë°˜ ì¤‘ ì„¸ë¶€ ë¶„ë¥˜\n",
        "        stage2_logits = self.stage2_classifier(cls_output)\n",
        "\n",
        "        loss = None\n",
        "        if binary_labels is not None and multiclass_labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss1 = loss_fct(stage1_logits, binary_labels)\n",
        "\n",
        "            mask = (binary_labels == 0)\n",
        "            if mask.sum() > 0:\n",
        "                loss2 = loss_fct(stage2_logits[mask], multiclass_labels[mask])\n",
        "                loss = loss1 + loss2\n",
        "            else:\n",
        "                loss = loss1\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=stage1_logits\n",
        "        )"
      ],
      "metadata": {
        "id": "s4-3AuysmLvn"
      },
      "id": "s4-3AuysmLvn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b807559",
      "metadata": {
        "id": "1b807559"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(config[\"tokenizer_name\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c36e166",
      "metadata": {
        "id": "0c36e166"
      },
      "source": [
        "## ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ í´ë˜ìŠ¤(í† í¬ë‚˜ì´ì§•)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f01ade78",
      "metadata": {
        "id": "f01ade78"
      },
      "outputs": [],
      "source": [
        "class TwoStageDataset(Dataset):\n",
        "    def __init__(self, encodings, binary_labels, multiclass_labels):\n",
        "        self.encodings = encodings\n",
        "        self.binary_labels = binary_labels\n",
        "        self.multiclass_labels = multiclass_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.binary_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.binary_labels[idx])\n",
        "        item[\"binary_labels\"] = torch.tensor(self.binary_labels[idx])\n",
        "        item[\"multiclass_labels\"] = torch.tensor(self.multiclass_labels[idx])\n",
        "        return item"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6386e17e",
      "metadata": {
        "id": "6386e17e"
      },
      "source": [
        "## ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4425cb18",
      "metadata": {
        "id": "4425cb18"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def prepare_two_stage_dataset(texts, labels, tokenizer, max_length=128, test_size=0.2):\n",
        "    # Stage 1 labels: 0~3 â†’ 0, 4 â†’ 1\n",
        "    binary_labels = [0 if l in [0, 1, 2, 3] else 1 for l in labels]\n",
        "\n",
        "    # Stage 2 labels: 0~3ì€ ê·¸ëŒ€ë¡œ, 4ëŠ” -1\n",
        "    multiclass_labels = [l if l in [0, 1, 2, 3] else -1 for l in labels]\n",
        "\n",
        "    # Split ë¨¼ì €\n",
        "    train_texts, val_texts, train_binary, val_binary, train_multi, val_multi = train_test_split(\n",
        "        texts, binary_labels, multiclass_labels, test_size=test_size, stratify=labels, random_state=42\n",
        "    )\n",
        "\n",
        "    # Tokenize ê° split\n",
        "    train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length,return_tensors=\"pt\" )\n",
        "    val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=max_length,return_tensors=\"pt\" )\n",
        "\n",
        "    # Dataset ìƒì„±\n",
        "    train_dataset = TwoStageDataset(train_encodings, train_binary, train_multi)\n",
        "    val_dataset = TwoStageDataset(val_encodings, val_binary, val_multi)\n",
        "\n",
        "    return train_dataset, val_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c8a89a0",
      "metadata": {
        "id": "5c8a89a0"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(config[\"train_file\"])\n",
        "texts = train_df[\"clean_text\"].tolist()\n",
        "labels = train_df[\"label\"].tolist()\n",
        "\n",
        "daily_df = pd.read_csv(config[\"daily_file\"])\n",
        "texts.extend(daily_df[\"clean_text\"].tolist())\n",
        "labels.extend([4 for _ in daily_df[\"label\"].tolist()])\n",
        "\n",
        "\"\"\"gen1_df = pd.read_csv(config[\"gen1_file\"])\n",
        "texts.extend(gen1_df[\"clean_text\"].tolist())\n",
        "labels.extend([4 for _ in gen1_df[\"label\"].tolist()])\n",
        "\n",
        "gen2_df = pd.read_csv(config[\"gen2_file\"])\n",
        "texts.extend(gen2_df[\"clean_text\"].tolist())\n",
        "labels.extend([4 for _ in gen2_df[\"label\"].tolist()])\"\"\"\n",
        "\n",
        "#ai_daily_df = pd.read_csv(config[\"aihub_daily_file\"])\n",
        "#texts.extend(ai_daily_df[\"conversation\"].tolist())\n",
        "#labels.extend([4 for _ in ai_daily_df[\"class\"].tolist()])\n",
        "\n",
        "#dataset = ChatDataset(texts, labels, tokenizer, config[\"max_length\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41bec869",
      "metadata": {
        "id": "41bec869"
      },
      "outputs": [],
      "source": [
        "train_dataset, val_dataset = prepare_two_stage_dataset(texts, labels, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ec2d5c3",
      "metadata": {
        "scrolled": true,
        "id": "9ec2d5c3"
      },
      "outputs": [],
      "source": [
        "label_counts = pd.Series(labels).value_counts().sort_index()\n",
        "\n",
        "print(\"ğŸ“Š train label í´ë˜ìŠ¤ ë¶„í¬:\")\n",
        "print(label_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9a135a6",
      "metadata": {
        "id": "e9a135a6"
      },
      "outputs": [],
      "source": [
        "\"\"\"label_counts = pd.Series(val_labels).value_counts().sort_index()\n",
        "\n",
        "print(\"ğŸ“Š validation label í´ë˜ìŠ¤ ë¶„í¬:\")\n",
        "print(label_counts)\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa151219",
      "metadata": {
        "id": "aa151219"
      },
      "source": [
        "## ë§¤íŠ¸ë¦­ í•¨ìˆ˜"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def evaluate_stage1(trainer, val_dataset):\n",
        "    pred_output = trainer.predict(val_dataset)\n",
        "    y_true = pred_output.label_ids\n",
        "    y_pred = np.argmax(pred_output.predictions, axis=1)\n",
        "\n",
        "    # ë©”íŠ¸ë¦­ ê³„ì‚°\n",
        "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    # ë¶„í¬ í™•ì¸\n",
        "    print(\"ğŸ“Š Stage 1 ì •ë‹µ ë¶„í¬:\", Counter(y_true))\n",
        "    print(\"ğŸ“Š Stage 1 ì˜ˆì¸¡ ë¶„í¬:\", Counter(y_pred))\n",
        "    print(\"âœ… Stage 1 Accuracy:\", acc)\n",
        "    print(\"âœ… Stage 1 F1 (macro):\", f1)\n",
        "\n",
        "    return {\n",
        "        \"f1_macro\": f1,\n",
        "        \"accuracy\": acc\n",
        "    }"
      ],
      "metadata": {
        "id": "Lp2eNYV9qY2D"
      },
      "id": "Lp2eNYV9qY2D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def evaluate_stage2(trainer, val_dataset):\n",
        "    # ì˜ˆì¸¡\n",
        "    pred_output = trainer.predict(val_dataset)\n",
        "    y_true = pred_output.label_ids\n",
        "    y_pred = np.argmax(pred_output.predictions, axis=1)\n",
        "\n",
        "    # ë©”íŠ¸ë¦­ ê³„ì‚°\n",
        "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    # ë¶„í¬ í™•ì¸\n",
        "    print(\"ğŸ“Š ì •ë‹µ ë¶„í¬:\", Counter(y_true))\n",
        "    print(\"ğŸ“Š ì˜ˆì¸¡ ë¶„í¬:\", Counter(y_pred))\n",
        "    print(\"âœ… Stage 2 Accuracy:\", acc)\n",
        "    print(\"âœ… Stage 2 F1 (macro):\", f1)\n",
        "\n",
        "    return {\n",
        "        \"f1_macro\": f1,\n",
        "        \"accuracy\": acc\n",
        "    }\n"
      ],
      "metadata": {
        "id": "mbfXhf3-IVxo"
      },
      "id": "mbfXhf3-IVxo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0d16d7e",
      "metadata": {
        "id": "a0d16d7e"
      },
      "outputs": [],
      "source": [
        "\"\"\"def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1_macro\": f1_score(labels, preds, average=\"macro\")\n",
        "    }\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "\n",
        "    # ì˜ˆì™¸ ì²˜ë¦¬: label_idsê°€ tupleì´ë©´ ì²« ë²ˆì§¸ ìš”ì†Œë§Œ ì‚¬ìš© (binary classificationìš©)\n",
        "    label_ids = pred.label_ids[0] if isinstance(pred.label_ids, tuple) else pred.label_ids\n",
        "\n",
        "    preds = np.argmax(pred.predictions, axis=1)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(label_ids, preds),\n",
        "        \"f1_macro\": f1_score(label_ids, preds, average=\"macro\")\n",
        "    }"
      ],
      "metadata": {
        "id": "W8UMwRlbszIh"
      },
      "id": "W8UMwRlbszIh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e4eef9cb",
      "metadata": {
        "id": "e4eef9cb"
      },
      "source": [
        "## ëª¨ë¸ ì¤€ë¹„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a610bfd3",
      "metadata": {
        "id": "a610bfd3"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./Aiffel_DLThon/ckpoints/tunib',\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=32,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=float(2e-5),\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_dir='./Aiffel_DLThon//logs/klue_roberta',\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=50,\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "from transformers import AutoConfig\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"tunib/electra-ko-en-base\")\n",
        "two_stage_model = ElectraTwoStageModel(config)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=two_stage_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[\n",
        "        EarlyStoppingCallback(early_stopping_patience=2),\n",
        "        LoggingCallback(),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c28e509d",
      "metadata": {
        "id": "c28e509d"
      },
      "source": [
        "## ëª¨ë¸ í•™ìŠµ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afd0a20e",
      "metadata": {
        "id": "afd0a20e"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "#save_model(two_stage_model, config[\"output_dir\"], epoch=config[\"epochs\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "model = two_stage_model\n",
        "class PlainTextDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length):\n",
        "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "\n",
        "test_df = pd.read_csv(\"./Aiffel_DLThon/data/original_data/test.csv\")\n",
        "test_texts = test_df[\"text\"].tolist()\n",
        "\n",
        "# Dataset & Dataloader\n",
        "test_dataset = PlainTextDataset(test_texts, tokenizer, 200)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "import torch\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "model.eval()\n",
        "label4_probs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        probs = softmax(outputs.logits, dim=-1)\n",
        "        label4_batch_probs = probs[:, 1].cpu().numpy()\n",
        "        label4_probs.extend(label4_batch_probs)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.histplot(label4_probs, bins=30, kde=True)\n",
        "plt.title(\"Distribution of Label 4 Probabilities on Test Set\")\n",
        "plt.xlabel(\"Predicted Probability of Label 4\")\n",
        "plt.ylabel(\"Number of Samples\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "a-_hdMBgLrmJ"
      },
      "id": "a-_hdMBgLrmJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "def evaluate_stage1(model, dataset, batch_size=32):\n",
        "    model.eval()\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "    preds = []\n",
        "    labels = []\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Stage 1 Evaluation\"):\n",
        "        input_ids = batch[\"input_ids\"].to(model.device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(model.device)\n",
        "        token_type_ids = batch[\"token_type_ids\"].to(model.device)\n",
        "        binary_labels = batch[\"binary_labels\"].to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                token_type_ids=token_type_ids\n",
        "            ).logits  # âœ… Stage 1 logits only\n",
        "\n",
        "        pred = torch.argmax(logits, dim=1)\n",
        "        preds.extend(pred.cpu().tolist())\n",
        "        labels.extend(binary_labels.cpu().tolist())\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='macro')\n",
        "\n",
        "    print(\"ğŸ“Š Stage 1 Evaluation Results\")\n",
        "    print(f\"âœ… Accuracy: {acc:.4f}\")\n",
        "    print(f\"âœ… F1 Score (macro): {f1:.4f}\")\n",
        "    print(\"ğŸ“‹ Classification Report:\\n\", classification_report(labels, preds, digits=4))\n",
        "    return {\"accuracy\": acc, \"f1_macro\": f1}\n",
        "stage1_metrics = evaluate_stage1(two_stage_model, val_dataset)\n"
      ],
      "metadata": {
        "id": "zHLW9LQAr-jv"
      },
      "id": "zHLW9LQAr-jv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "binary_preds = np.argmax(trainer.predict(val_dataset).predictions, axis=1)\n",
        "counter = Counter(binary_preds)\n",
        "print(\"âœ… Stage 1 ì˜ˆì¸¡ ë¶„í¬:\", counter)"
      ],
      "metadata": {
        "id": "NSy6cP5_D2KP"
      },
      "id": "NSy6cP5_D2KP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stage2_df = train_df[train_df[\"label\"] != 4].copy()\n",
        "stage2_df = stage2_df.reset_index(drop=True)\n",
        "\n",
        "texts = stage2_df[\"clean_text\"].tolist()\n",
        "labels = stage2_df[\"label\"].tolist()  # label: 0~3\n",
        "\n",
        "encodings = tokenizer(texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
        "class Stage2Dataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts, labels, test_size=0.2, stratify=labels, random_state=42\n",
        ")\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n",
        "train_dataset = Stage2Dataset(train_encodings, train_labels)\n",
        "val_dataset = Stage2Dataset(val_encodings, val_labels)\n",
        "\n",
        "from transformers import ElectraModel, ElectraPreTrainedModel\n",
        "import torch.nn as nn\n",
        "\n",
        "class ElectraStage2Classifier(ElectraPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.electra = ElectraModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, 4)  # 4-class\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "        cls_output = self.dropout(outputs.last_hidden_state[:, 0, :])\n",
        "        logits = self.classifier(cls_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(logits, labels)\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits\n",
        "        )\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./stage2_model\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def compute_metrics_stage2(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1_macro\": f1_score(labels, preds, average=\"macro\")\n",
        "    }\n",
        "from transformers import AutoConfig\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"tunib/electra-ko-en-base\", num_labels=4)\n",
        "model = ElectraStage2Classifier.from_pretrained(\"tunib/electra-ko-en-base\", config=config)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics_stage2\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "M6lodAgmEfBU"
      },
      "id": "M6lodAgmEfBU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ì‹¤ì œ ì‚¬ìš©í•œ label ë¦¬ìŠ¤íŠ¸ë¥¼ ë„£ìœ¼ì„¸ìš”\n",
        "# ì˜ˆ: labels = stage2_df[\"label\"].tolist()\n",
        "stage2_labels = labels  # âœ… â† ì—¬ê¸°ë§Œ ìˆ˜ì •\n",
        "\n",
        "# ë¶„í¬ í™•ì¸\n",
        "label_counts = Counter(stage2_labels)\n",
        "\n",
        "print(\"ğŸ“Š Stage 2 í•™ìŠµìš© ë¼ë²¨ ë¶„í¬:\")\n",
        "for label, count in sorted(label_counts.items()):\n",
        "    print(f\"Label {label}: {count}\")\n",
        "\n",
        "# ì‹œê°í™”\n",
        "plt.bar(label_counts.keys(), label_counts.values())\n",
        "plt.title(\"Stage 2 Class Distribution\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks([0, 1, 2, 3])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Nw13wpHwFWYa"
      },
      "id": "Nw13wpHwFWYa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e11778cb",
      "metadata": {
        "id": "e11778cb"
      },
      "source": [
        "## í…ŒìŠ¤íŠ¸"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "batch_size = 32  # ë˜ëŠ” 16ìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆìŒ\n",
        "two_stage_model.eval()\n",
        "model.eval()\n",
        "final_preds = []\n",
        "test_df = pd.read_csv(\"./Aiffel_DLThon/data/original_data/test.csv\")\n",
        "test_texts = test_df[\"text\"].tolist()\n",
        "\n",
        "for i in tqdm(range(0, len(test_texts), batch_size), desc=\"Stage 1 + Stage 2 ì¶”ë¡ \"):\n",
        "    batch_texts = test_texts[i:i + batch_size]\n",
        "\n",
        "    # ì¸ì½”ë”© + device ì´ë™\n",
        "    batch_encodings = tokenizer(\n",
        "        batch_texts,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    batch_encodings = {k: v.to(two_stage_model.device) for k, v in batch_encodings.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Stage 1 ì˜ˆì¸¡\n",
        "        s1_logits = two_stage_model(**batch_encodings).logits\n",
        "        s1_preds = torch.argmax(s1_logits, dim=1)\n",
        "\n",
        "        for j, s1 in enumerate(s1_preds):\n",
        "            if s1 == 1:\n",
        "                final_preds.append(4)\n",
        "            else:\n",
        "                # Stage 2 ì˜ˆì¸¡\n",
        "                single_input = {\n",
        "                    key: val[j].unsqueeze(0).to(model.device)\n",
        "                    for key, val in batch_encodings.items()\n",
        "                }\n",
        "                with torch.no_grad():\n",
        "                    s2_logits = model(**single_input).logits\n",
        "                    s2_pred = torch.argmax(s2_logits, dim=1).item()\n",
        "                    final_preds.append(s2_pred)"
      ],
      "metadata": {
        "id": "7XABiwcuL0gM"
      },
      "id": "7XABiwcuL0gM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(\"ğŸ“Š ì˜ˆì¸¡ ê²°ê³¼ ë¶„í¬:\")\n",
        "print(Counter(final_preds))"
      ],
      "metadata": {
        "id": "3FHHhpJywyZR"
      },
      "id": "3FHHhpJywyZR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    print(f\"ë¬¸ì¥: {test_texts[i]}\")\n",
        "    print(f\"ì˜ˆì¸¡ ë¼ë²¨: {final_preds[i]}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "-kEeVRMzwyO7"
      },
      "id": "-kEeVRMzwyO7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04bddf26",
      "metadata": {
        "id": "04bddf26"
      },
      "outputs": [],
      "source": [
        "save_submission(preds, \"./Aiffel_DLThon/data/original_data/submission.csv\", './submission.csv')\n",
        "log_message(\"Experiment complete.\", '.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, ElectraPreTrainedModel, ElectraModel\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ============================\n",
        "# âœ… ëª¨ë¸ ì •ì˜ (í†µí•© ëª¨ë¸)\n",
        "# ============================\n",
        "class ElectraTwoStageModel(ElectraPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.electra = ElectraModel(config)\n",
        "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.stage1_classifier = torch.nn.Linear(config.hidden_size, 2)\n",
        "        self.stage2_classifier = torch.nn.Linear(config.hidden_size, 4)\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None,\n",
        "                binary_labels=None, multiclass_labels=None):\n",
        "        outputs = self.electra(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        cls_output = self.dropout(outputs.last_hidden_state[:, 0, :])\n",
        "\n",
        "        stage1_logits = self.stage1_classifier(cls_output)\n",
        "        stage2_logits = self.stage2_classifier(cls_output)\n",
        "\n",
        "        loss = None\n",
        "        if binary_labels is not None and multiclass_labels is not None:\n",
        "            loss_fct = torch.nn.CrossEntropyLoss()\n",
        "            loss1 = loss_fct(stage1_logits, binary_labels)\n",
        "            mask = (binary_labels == 0)\n",
        "            if mask.sum() > 0:\n",
        "                loss2 = loss_fct(stage2_logits[mask], multiclass_labels[mask])\n",
        "                loss = loss1 + loss2\n",
        "            else:\n",
        "                loss = loss1\n",
        "\n",
        "        return SequenceClassifierOutput(loss=loss, logits=stage1_logits)\n",
        "\n",
        "\n",
        "# ============================\n",
        "# âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "# ============================\n",
        "from transformers import AutoConfig\n",
        "MODEL_NAME = \"tunib/electra-ko-en-base\"\n",
        "MODEL_PATH = \"tunib/electra-ko-en-base\"  # í•™ìŠµëœ ëª¨ë¸ì´ ì €ì¥ëœ ë””ë ‰í† ë¦¬\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
        "model = ElectraTwoStageModel.from_pretrained(MODEL_PATH, config=config).to(\"cuda\")\n",
        "model.eval()\n",
        "\n",
        "# ============================\n",
        "# âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n",
        "# ============================\n",
        "test_df = pd.read_csv(\"./Aiffel_DLThon/data/original_data/test.csv\")\n",
        "test_texts = test_df[\"text\"].tolist()\n",
        "\n",
        "# ============================\n",
        "# âœ… ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "# ============================\n",
        "final_preds = []\n",
        "\n",
        "for text in tqdm(test_texts):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Stage 1 ì˜ˆì¸¡\n",
        "        stage1_logits = model(**inputs).logits\n",
        "        stage1_pred = torch.argmax(stage1_logits, dim=1).item()\n",
        "\n",
        "        if stage1_pred == 1:\n",
        "            final_preds.append(4)\n",
        "        else:\n",
        "            # Stage 2 ì˜ˆì¸¡\n",
        "            cls_output = model.dropout(model.electra(**inputs).last_hidden_state[:, 0, :])\n",
        "            stage2_logits = model.stage2_classifier(cls_output)\n",
        "            stage2_pred = torch.argmax(stage2_logits, dim=1).item()\n",
        "            final_preds.append(stage2_pred)\n",
        "\n",
        "# ============================\n",
        "# âœ… ì œì¶œ íŒŒì¼ ì €ì¥\n",
        "# ============================\n",
        "submission = pd.DataFrame({\n",
        "    \"idx\": test_df[\"idx\"],\n",
        "    \"label\": final_preds\n",
        "})\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "print(\"\\nâœ… submission.csv ì €ì¥ ì™„ë£Œ!\")\n"
      ],
      "metadata": {
        "id": "QxxizLPgPqFk"
      },
      "id": "QxxizLPgPqFk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_target_distribution(df: pd.DataFrame, log_dir: str):\n",
        "    \"\"\"\n",
        "    ì£¼ì–´ì§„ DataFrameì—ì„œ 'target' ì»¬ëŸ¼ì˜ í´ë˜ìŠ¤ ë¶„í¬ë¥¼ ë¡œê·¸ íŒŒì¼ê³¼ ì½˜ì†”ì— ì¶œë ¥í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    if \"label\" not in df.columns:\n",
        "        log_message(\"âŒ 'target' ì»¬ëŸ¼ì´ ë°ì´í„°í”„ë ˆì„ì— ì—†ìŠµë‹ˆë‹¤.\", log_dir)\n",
        "        return\n",
        "\n",
        "    counts = df[\"label\"].value_counts().sort_index()\n",
        "    log_message(\"ğŸ“Š Target í´ë˜ìŠ¤ ë¶„í¬:\", log_dir)\n",
        "    for label, count in counts.items():\n",
        "        log_message(f\"Label {label}: {count}\", log_dir)"
      ],
      "metadata": {
        "id": "npymFbF7QUDr"
      },
      "id": "npymFbF7QUDr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fe38c92",
      "metadata": {
        "scrolled": true,
        "id": "4fe38c92"
      },
      "outputs": [],
      "source": [
        "# íŒŒì¼ ë¡œë”©\n",
        "df = pd.read_csv('/content/submission.csv')\n",
        "log_target_distribution(df, '.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2388cde",
      "metadata": {
        "id": "c2388cde"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}