pretrained_model_name: klue/bert-base
tokenizer_name: klue/bert-base

num_labels: 5
max_length: 200

batch_size: 32
epochs: 3
learning_rate: 2e-5
weight_decay: 0.01
warmup_ratio: 0.1
lr_scheduler_type: linear
gradient_clip_value: 1.0

save_total_limit: 2
eval_strategy: epoch
save_strategy: epoch

train_file: ../data/processed/clean_train.csv
test_file: ../data/processed/clean_test.csv
submission_file: ../data/results/klue_bert_dailyfull.csv
output_dir: ../ckpoints/klue_bert
log_dir: ../logs/klue_bert

experiment_name: klue_bert_exp1
use_wandb: false
