pretrained_model_name: beomi/kcbert-base
tokenizer_name: beomi/kcbert-base

num_labels: 5
max_length: 200

batch_size: 32
epochs: 3
learning_rate: 2e-5
weight_decay: 0.01
warmup_ratio: 0.1
lr_scheduler_type: linear
gradient_clip_value: 1.0

save_total_limit: 2
eval_strategy: epoch
save_strategy: epoch

train_file: ../data/processed/clean_train.csv
test_file: ../data/processed/clean_test.csv
submission_file: ../data/results/kcbert_dailyfull.csv
output_dir: ../ckpoints/kcbert
log_dir: ../logs/kcbert

experiment_name: kcbert_exp1
use_wandb: false
