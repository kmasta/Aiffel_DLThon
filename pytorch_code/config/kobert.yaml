pretrained_model_name: monologg/kobert
tokenizer_name: monologg/kobert

num_labels: 5
max_length: 200

batch_size: 32
epochs: 3
learning_rate: 2e-5
weight_decay: 0.01
warmup_ratio: 0.1
lr_scheduler_type: linear
gradient_clip_value: 1.0

save_total_limit: 2
eval_strategy: epoch
save_strategy: epoch

train_file: ../data/processed/clean_train.csv
test_file: ../data/processed/clean_test.csv
submission_file: ../data/results/kobert_dailyfull.csv
output_dir: ../ckpoints/kobert
log_dir: ../logs/kobert

experiment_name: kobert_exp1
use_wandb: false
